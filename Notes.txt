###############
ToDo (Immediately):

	ADDITIONAL OUTPUT: COMBINE THE FINAL LIST AND THE GAZATTEER
		i.e., a potentially imperfect but maximal list of in+out corpus NE’s

	ALIGN OUTPUT BACK TO PREPARED INPUT AND ADD ID COLUMN
		id specifies at which step each word was annotated
			(seed, 1,2,…n, auto) where auto means never manually annotated
		This is for David’s analysis of from which texts can we learn the most
			about the behavior of NE’s in the full corpus and how does
			this compare to the simpler question of from which texts do we
			get the most NE’s or the highest NE density?

###############
ToDo (Some Day):

	More formal evaluation of preTag_delex and comparison to Random and HardCappedUNKs

	ALTER PREPROCESSING STEP to preserve features, not just labels
		i.e., metadata from the OCR output html or previously existing linguistic analysis
		The issue is analyzers and disambiguators will apply their own tokenization which will have to be aligned

	SCRIPT TO MAP OUTPUT BACK TO ORIGINAL RAW INPUT .TXT OR .XML FILES

	Move the structured preprocessing over to beautiful soup, no reason to still be using such hacky code for this

###################################################################################

##############
9/28/18 - Alex

Fixed the misalignment during evaluation bug and fixed the bug in cross validation that was artificially boosting the scores

##############
9/26/18 - Alex

Fixed small bug with gazatteer usage
	Verified that multiple gazatteers can be leveraged even if some or all do not exactly match a predefined label
Ran initial experiment on French corpus to see the effect of removing capitalization
	preTag-delex showed about 50% Error Reduction over hardCappedUNKs algorithm 

##############
9/23/18 - Alex

Changed name of system to HER, Humanities Entity Recognizer (or Herodotos Entity Recognizer) — Thanks David for the name!

Wrote the README and addressed some minor bugs, added some minor functionality, all covered in said README

Product is ready for release, though the to-do list should still be addressed even though it shouldn’t affect the users’ results in any major why when using HER


##############
9/21/18 - Alex

Finished coding the language independent algorithm preTag_deLex
	Seems to work really well but could use some more betaTesting

Stored Data and Gazetteers for DH paper with David and Beatrice locally
	/Users/AE/Documents/SideProjects/Agnostic_NER/Paris_collaboration/ENS_followUp_09-18/

Pulled gazetteers from Beatrice’s dump
	for i in {1..67}; do header="$(head -1 Artlas_export_all_exhibited_works.tsv | cut -f$i)"; header="$(echo $header | tr -s ' ' | tr ' ' '_')"; tail -n +2 Artlas_export_all_exhibited_works.tsv | cut -f$i | sort -u > $header.gaz; done


##############
9/13/18 - Alex

	Downloaded Greek and Latin data from Perseus Github Repositories
		https://github.com/PerseusDL/canonical

	Put all the relevant Greek or Latin files in one directory
		renamed them something sensible

		mkdir GREEK # or LATIN
		for f in <cloned-greek-perseus-repository>/data/*/*grc*.xml; mv $f DATA/.; done
			# use lat instead of grc for Latin
		python Scripts/Perseus_Title_Author_Extractor.sh GREEK/* # or LATIN

		then I divided GREEK and LATIN into two smaller files and compressed them to share easily

	I tested HER preprocessing on the Latin and Greek data


