
##############
9/13/18 - Alex

	Downloaded Greek and Latin data from Perseus Github Repositories
		https://github.com/PerseusDL/canonical

	Put all the relevant Greek or Latin files in one directory
		renamed them something sensible

		mkdir GREEK # or LATIN
		for f in <cloned-greek-perseus-repository>/data/*/*grc*.xml; mv $f DATA/.; done
			# use lat instead of grc for Latin
		python Scripts/Perseus_Title_Author_Extractor.sh GREEK/* # or LATIN

		then I divided GREEK and LATIN into two smaller files and compressed them
			so they were small enough that I could move them to Data/ and upload to Github

	I cloned our AgnosticNER github repository to test it with the new data

		Step 1 in the README will obviously not work well because the MOSES preprocessor doesn't handle Latin or Greek, so we shouldn't run:

		sh Scripts/prepare_original_texts.sh Scripts/preprocess.py $lg 

		but the first command in that Step 1 (python Scripts/preprocess.py <file>) seems to be successfully extracting both Latin and Greek body texts from the xml

			if we find errors with this later, we could try using the beautiful soup package instead

##############
9/21/18 - Alex

Finished coding the language independent algorithm preTag_deLex
	Seems to work really well but could use some more betaTesting

Stored Data and Gazetteers for DH paper with David and Beatrice locally
	/Users/AE/Documents/SideProjects/Agnostic_NER/Paris_collaboration/ENS_followUp_09-18/

Pulled gazetteers from Beatrice’s dump
	for i in {1..67}; do header="$(head -1 Artlas_export_all_exhibited_works.csv | cut -f$i -d";")"; header="$(echo $header | tr -s ' ' | tr ' ' '_')"; tail -n +2; Artlas_export_all_exhibited_works.csv | cut -f$i -d";" | sort -u > portuguese.$header.gaz; done


###################################################################################
###############
ToDo (Immediately):

	UPDATE README -> after each annotation session, record the results at that point for ex-post analysis and invite the user to check their progress and decide how much future annotation is needed by eyeballing output
		FIX REPORTING OF ACCURACY PREDICTION 
	
	SEPARATE PUNCTUATION IN GAZETTEERS I PULLED FROM BEATRICE’S DUMP

	ALTER PREPROCESSING STEP
		cut off maximize length of sentences at 200
		to preserve features, not just labels, i.e., metadata from the OCR output html or previously existing linguistic analysis

	CHANGE DEFAULT EVALUATION TO BE ( LIST RECALL + IN-CONTEXT F ) / 2 (also how it's calculated and reported in oracle.py)

	COMBINE THE FINAL LIST AND THE GAZATTEER AT THE END TO MAKE A POTENTIALLY IMPERFECT POTENTIALLY LONGER GAZATTEER

###############
ToDo (Some Day):

	SCRIPT TO MAP OUTPUT BACK TO ORIGINAL INPUT .TXT OR .XML FILES

	Custom Eval Bug may still be around, where you have to realign the predictions file by removing an empty first line if you’re unlucky enough to get one
